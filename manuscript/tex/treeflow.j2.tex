\BLOCK{extends template}

\BLOCK{block preamble}
\usepackage{booktabs}
\BLOCK{endblock}

\BLOCK{block abstract}
This is the abstract
\BLOCK{endblock}

\BLOCK{block body}
\section{Introduction}

% Motivate probabilistic programming/phylogenetics
Traditionally, phylogenetic analyses have been performed by specialist software. A number of software packages exist that implement a broad but predefined collection of models and associated specialized inference methods. Typically, inference is handled by carefully crafted but computationally costly stochastic optimisation or Markov Chain Monte Carlo (MCMC) methods. In contrast, in other realms of statistical analysis, \textit{probabilistic programming} software libraries have entered into widespread use. These allow the specification of almost any model as a probabilistic program, and inference is provided automatically with a generic inference method. Having the power of probabilistic programming in phylogenetic analyses could significantly accelerate research. % TODO: Citations for software libraries

% Review probabilistic programming
Probabilistic programming tools, such as BUGS \cite{lunn2000winbugs}, Stan \cite{carpenter2017stan}, PyMC3 \cite{salvatier2016probabilistic}, Pyro \cite{bingham2019pyro}, and TensorFlow Probability \cite{dillon2017tensorflow} allow users to specify probabilistic models by describing the generative process with code as a \textit{probabilistic program}. Advancements in automatic inference methods have allow these tools to perform efficient inference on almost any model. Some of notable examples, including Automatic Differentiation Variational Inference \cite{kucukelbir2017automatic} (ADVI) and Hamiltonian Monte Carlo \cite{duane1987hybrid}, use local information gradient information from the model's probability density function to efficiently navigate the parameter space.

One key technology that enables these gradient-based automatic inference algorithms is \textit{automatic differentiation}. Automatic differentiation refers to methods which efficiently calculate machine-precision gradients of functions, specified by computer programs, without extra analytical derivation or excessive computational overhead compared to the function evaluation. Automatic differentiation frameworks that have statistical inference packages built on top of them include Theano \cite{bergstra2010theano}, Pytorch \cite{paszke2019pytorch}, JAX \cite{jax2018github} and TensorFlow \cite{abadi2016tensorflow}. Some of these extend to non-trivial computational constructs such as control flow and recursion \cite{yu2018dynamic}.

The structure of the phylogenetic tree object is a major barrier to implementing probabilistic programming for phylogenetics. It is not clear how the association between its discrete and continuous quantities (the topology and branch lengths respectively) should be represented and handled in inference. Also, the combinatorial explosion of the size of the phylogenetic state space presents a major challenge to any inference algorithm. Generic random search methods for discrete variables, as in the naive implementation of MCMC sampling, do not scale appropriately to allow inference on modern datasets with thousands of sequences.

One core computation that presents a challenge to automatic differentiation is the \textit{phylogenetic likelihood}. This computes the probability function of a collection of sequences given a phylogenetic tree, integrating out the character states at ancestral nodes \cite{felsenstein1981evolutionary}. This is most efficiently performed through dynamic programming, which requires sequential control flow or recursion, which are not always trivial to implement in a functional automatic differentiation framework such as TensorFlow. Additionally, since the computational cost scales linearly with the number of sequences, naively computing the likelihood's gradient with respect to each branch of the tree yields a quadratic computational cost \cite{ji2020gradients}.


% TODO: Existing work - VBPI, PhyloStan, PPHMC, zhang2020improved (normalising flows)

\section{Description}

TreeFlow is a library for probabilistic programming in Python. It is built on TensorFlow, a computational framework for machine learning. TreeFlow leverages Tensorflow's capabilities for accelerated numerical computation and automatic differentiation. It leverages the existing probabilistic modelling infrastructure provided by TensorFlow Probability, which implements standard statistical distributions and inferential machinery \cite{dillon2017tensorflow}. TreeFlow provides a phylogenetic tree representation for TensorFlow and associated input and output methods, a range of widely used phylogenetic distributions and functions, and tools for applying modern statistical inference methods to phylogenetic models.
% TODO: Flesh out this first paragraph

TensorFlow's core computational object is the Tensor, a multi-dimensional array with a uniform data type. Phylogenetic trees are not immediately at home in a Tensor-centric universe, as they are a complex data structure, often defined recursively, with both continuous and discrete components. TreeFlow represents trees as a structure of Tensors; floating point Tensors representing the times and branch lengths, and integer Tensors representing the topology. Common tree operations such as traversals and navigating to ancestor nodes can be performed through these integer Tensors. TensorFlow has extensive support for "nested" structures of Tensors, including using them as arguments to compiled functions and defining distributions over complex objects. This means computations and models involving phylogenetic trees can be expressed naturally.

A range of phylogenetic distributions and models are implemented in TreeFlow. These are primarily generative models for phylogenetic trees and models of sequence evolution. Generative models of phylogenetic trees, such as Kingman's coalescent \cite{kuhner1995estimating} and Birth-Death-Sampling speciation processes \cite{stadler2009incomplete}, can be used to infer parameters related to population dynamics from genetic sequence data. TreeFlow implements models of nucleotide sequence evolution such as the HKY85 \cite{hasegawa1985dating} and General Time Reversible (GTR) \cite{tavare1986some}. It includes a standard approach for dealing with heterogeneity in mutation rate across sites based on a marginalizing over a discretized site rate distribution \cite{yang1994maximum}. The probabilistic programming framework, however, allows for the use of any appropriate distribution as the base site rate distribution rather than just the standard single-parameter Gamma distribution. For example, it is straightforward to replace the base Gamma distribution with a Weibull distribution, which has a quantile function that is much easier to compute \cite{fourment2019evaluating}. Thanks to TensorFlow's vectorized arithmetic operations, it is also natural to model variations in mutation rate over lineages by specifying parameters for multiple rates (possibly with a hierarchical prior) and multiplying by the branch lengths of the phylogenetic (time) tree. Models which can be naturally expressed this way include the log-Normal random local clock \cite{drummond2006relaxed} and auto-correlated relaxed clock models \cite{thorne1998estimating}.

% Phylogenetic likelihood 
A computation that requires special treatment is the \textit{phylogenetic likelihood}, the likelihood of a sequence alignment given a phylogeny and model of sequence evolution. This is typically involves integrating out character states at unsampled ancestral nodes using a \textit{dynamic programming} computation known as \textit{Felsenstein's pruning algorithm} \cite{felsenstein1981evolutionary}. The postorder tree traversal and dynamic data structure are not obviously compatible with Tensorflow's immutable data structures and focus on vectorized computations. Additionally, naive implementations result in gradient computations with problematic scaling. The computational cost of computing the derivatives of this likelihood with respect to all the branches of the phylogenetic tree could grow quadratically with respect to the number of taxa, and would prohibit gradient-based inference on large datasets \cite{ji2020gradients}. These issues are overcome in TreeFlow by implementing the dynamic programming data structure with TensorFlow's TensorArray construct \cite{yu2018dynamic}. The TensorArray is a data structure representing a collection of tensors which allows efficient implementation of the sequential computation. The \textit{write-once} property enforced on its constituent tensors ensures that gradient computations have appropriate scaling, as evidenced by the benchmarks below.

% Ratio transform
Another useful tool for phylogenetic inference implemented in TreeFlow is the \textit{node height ratio transform} \cite{kishino2001performance}. The ratio transform parametrizes the internal node heights of a tree as the ratio between a node's height and its parent's height. The heights can be computed from the ratios in a pre-order tree traversal. This transformation has a triangular Jacobian matrix, which means computing the determinant required for change of variable of a probability density can be computed in linear time with respect to the number of internal node heights \cite{fourment2019evaluating}. In combination with a log transformation of the root height and a logit transformation of the ratios, a multivariate distribution that takes real values can be transformed into a distribution on internal node heights of rooted phylogenies. This has been applied to phylogenetic inference in the context of automatic differentiation variational inference \cite{fourment2019evaluating} and Hamiltonian Monte Carlo \cite{ji2021scalable}. The ratio transform is implemented using a TensorArray-based computation as a TensorFlow Probability Bijector which provides a convenient interface for transforming real-valued distributions into phylogenetic tree distributions.

TensorFlow Probability distributions can be composed into a \textit{probabilistic graphical model} using TensorFlow Probability's joint distribution functionality \cite{piponi2020joint}. The code to specify a joint distribution provides a concise representation of the model used in a data analysis. The ability to implement phylogenetic models in this framework means that automatic inference algorithms implemented in TensorFlow can be leveraged. The discrete topology element of phylogenetic trees is an obstacle in the usage of these algorithms, which are typically restricted to continuous latent variables. Often, the phylogenetic tree topology is not the latent variable of interest, and is not a significant source of uncertainty. This is often the case when divergence times or other substitution or speciation model parameters are the focus. In these cases, useful results can be obtained by performing inference with a fixed tree topology, such as one obtained from fast maximum likelihood methods. TreeFlow provides tools for performing inference with a fixed tree topology using automatic inference algorithms.
% TODO: Source for fixed topology
% More on probabilistic programming?

% Inference
One form of statistical inference for which the gradient computation is essential is variational Bayesian inference \cite{jordan1999introduction}. The goal of Bayesian inference is to characterise a \textit{posterior distribution} which represents uncertainty over model parameters. Variational Bayesian inference achieves this by optimizing an approximation to the posterior distribution which has more convenient analytical properties. One concrete implementation of variational inference is \textit{automatic differentiation variational inference} (ADVI) \cite{kucukelbir2017automatic}. ADVI can perform inference on a wide range of probabilistic graphical models composed of continuous variables. It automatically constructs an approximation to the posterior by transforming a convenient base distribution to respect the domains of the model's component distributions. It then optimizes this approximation using stochastic gradient methods \cite{robbins1951stochastic, bottou2010large}. TreeFlow implements ADVI using TensorFlow Probability's bijector framework to transform a base distribution and leverages the stochastic gradient optimizers already implemented in TensorFlow. Tree variables are estimated by fixing the topology. The base distribution for the divergence times on the tree is transformed into a distribution on ratios using a logit transformation, and then into a valid set of divergence times using the ratio transformation described above. ADVI opens the door to using TensorFlow's neural network framework to implement deep-learning-based variants such as variational inference with \textit{normalizing flows} \cite{rezende2015variational}, which transform the base distribution through invertible trainable neural network layers to better approximate complex posterior distributions.

\section{Benchmarks}

% Methods

The phylogenetic likelihood is the computation that dominates the computational cost of model-based phylogenetic inference. We benchmark the performance of our TensorFlow-based likelihood implementation against specialized libraries. Since gradient computations are of equal importance to likelihood evaluations in modern automatic inference regimes, we also benchmark computation of derivatives of the phylogenetic likelihood, with respect to both the continuous elements of the tree and the parameters of the substitution model. A clear difference emerges in the implementation of derivatives; TreeFlow's are based on automatic differentiation while bespoke libraries need analytically-derived gradients. Therefore, we do not necessarily expect our implementation to be as fast as bespoke software, but it does not rely on analytical derivation of gradient expressions for every model and therefore automatically supports a wider range of models.

We compare the performance of TreeFlow's likelihood implementation against BEAGLE \cite{ayres2019beagle}. BEAGLE is library for high performance computation of phylogenetic likelihoods. Newer development branches implement analytical gradients with respect to tree branch lengths \cite{ji2020gradients}. We use BEAGLE via the \texttt{bito} software package \cite{bito}, which provides a Python interface to BEAGLE and also numerically computes derivatives with respect to substitution model parameters using a finite differences approximation.

We also compare TreeFlow with a simple likelihood implementation based on another automatic differentiation framework, JAX \cite{jax2018github}. In contrast to TensorFlow's function mode, which the benchmarked TreeFlow implementation uses, Jax uses an eager execution model and is compatible with native Python control flow.

Benchmarks are performed on simulated data. Simulation allows the generation of a large number of data replicates with appropriate properties. Since we want to investigate the scaling of likelihood and gradient calculations with respect to the number of sequences, we simulate data for a range of sequence counts. Sequence counts are selected as increasing powers of 2 to better display asymptotic properties of the implementations. We simulate data with sequence counts ranging from \VAR{min_sequence_count} to \VAR{max_sequence_count}. Trees are simulated under a coalescent model for a given number of taxa, and then nucleotide sequences of length \VAR{sequence_length} are simulated under a fixed rate of evolution and a HKY substitution model. Tree and sequence simulations are performed using BEAST 2 \cite{bouckaert2019beast}.

We benchmark 4 distinct computations on these datasets. Firstly, for each replicate we compute the phylogenetic likelihood under a very simple model of sequence evolution. This uses a JC model of nucleotide substitution and no model of site rate variation. Secondly, we calculate derivatives with respect to branch lengths under this simple model. Thirdly, we compute the likelihood under a more sophisticated substitution model, with a discretized Weibull distribution with 4 categories to model site rate variation and a GTR model of nucleotide substitution. We selected the Weibull distribution for site rate variation since it is implemented in \texttt{bito}. Finally, we compute derivatives with respect to both branch lengths and the parameters of the substitution model (the 6 GTR relative substitution rates, 4 base nucleotide frequencies, and the Weibull site rate shape parameter). Each computation is performed \VAR{sample_count} times on \VAR{replicate_count} simulated datasets.


\begin{figure}
    \centering
    \VAR{ figures("benchmark", "[width=\\linewidth]") }
    \caption{Benchmark results}
    \label{fig:benchmark}
\end{figure}

\begin{table}
    \centering
    \VAR{ tables["benchmark_summary"] }
    \caption{Log-log linear fit to benchmark times}
    \label{tab:benchmarkfit}
\end{table}

Figure \ref{fig:benchmark} shows the results of the benchmarks with a log scale on both axes. For likelihood computation with both models and branch gradient computation with the simple model \texttt{bito}/BEAGLE are at least an order of magnitude faster than TreeFlow. This is expected as BEAGLE is a highly optimized special-purpose library written in native code. The performance gap grows much smaller for computing the gradients of the more complex substitution model. \texttt{bito} performs at least 2 likelihood evaluations for each additional parameter when calculating the gradient with respect to substitution model parameters, while the overhead for substitution model parameters with automatic differentiation is minimal. We expect TreeFlow to surpass \texttt{bito}/BEAGLE for substitution models with even more parameters (e.g. codon models \cite{goldman1994codon}), or those where the number of parameters grows with the number of sequences, as in the example below.

We also observe that the runtimes for TreeFlow are roughly an order of magnitude less than those of the JAX-based implementation. These indicate that the control flow constructs and execution model of TensorFlow are a good choice for implementing tree-based computations compared to the eager execution model of JAX.

Table \ref{tab:benchmarkfit} shows the coefficients obtained from fitting a linear model to the benchmark times where the predictor is the log-transformed number of sequences and the target is the log-transformed runtime. The slope parameter estimates from these fits is a rough empirical estimate of the polynomial degree of the computational scaling. The slope parameters for all TreeFlow computations are well below 2 and indicate a roughly linear scaling with the size of the data, and certainly one that is not worse than the analytical linear-time gradient in \texttt{bito}/BEAGLE, indicating that the TensorArray-based likelihood implementation enables linear-time branch gradients.

\section{Biological examples}

We used TreeFlow to perform fixed-topology phylogenetic analyses of two datasets. The first is an alignment of 62 mitochondrial DNA sequences from carnivores. In both datasets maximum likelihood unrooted topologies are estimated using RAxML \cite{stamatakis2014raxml}. These topologies are rooted with Least Squares Dating \cite{to2016lsd}.

In the carnivores dataset, we demonstrate the flexibility of probabilistic programming with TreeFlow by investigating variation in the ratio of transitions to transversions in the nucleotide substitution process across lineages in the tree. Early maximum likelihood analyses detected variation in this ratio, but without a biological basis, attributed to a saturation effect \cite{yang1999estimation}. A later simulation-based investigation showed that this was a reasonable explanation \cite{duchene2015declining}.

\begin{figure}
    \centering
    \VAR{ figures("carnivores_marginals", "[width=\\linewidth]") }
    \caption{Carnivores base model marginal parameter estimates}
    \label{fig:carnivoresmarginals}
\end{figure}

This problem could be approached by means of Bayesian model comparison; a lack of preference for a model allowing between-lineage variation of the ratio could indicate that the substitution model lacks the power to separate variation `signal' from saturation `noise'. Firstly, we construct a standard phylogenetic model with a HKY substitution model with a single transition-transversion ratio parameter. We fit this model using ADVI, and also using MCMC as implemented in BEAST 2 \cite{bouckaert2019beast}. Figure \ref{fig:carnivoresmarginals} compares the marginal parameter estimates obtained from TreeFlow and BEAST 2.

We then altered this model to estimate a separate ratio for every lineage on the tree; the implementation of this was simple and scalable as a result of TensorFlow's vectorization and broadcasting functionality. We compare the models using estimates of the marginal likelihood \cite{mackay2003information}. The \textit{marginal likelihood}, or \textit{evidence}, the integral of the likelihood of the data over model parameters, is typically analytically intractable and challenging to compute using Markov Chain Monte Carlo methods \cite{xie2011improving}. The closed-form approximation to the posterior distribution provided by variational inference means we can easily estimate the marginal likelihood using importance sampling, a utility provided by TreeFlow. The marginal likelihood estimate for the model with transition-transversion ratio variation across lineages was lower. This means that, under the other components of this model, the data shows no evidence for variation in the ratio; any improvement in fit as a result of the extra parameters did not justify the increase in uncertainty they created.

% TODO: Include code
% TODO: Plots

% In the second dataset, we demonstrate the scalability of variational inference.

% TODO: Second dataset

\BLOCK{endblock}