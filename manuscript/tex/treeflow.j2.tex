\BLOCK{extends template}

\BLOCK{block preamble}
\BLOCK{endblock}

\BLOCK{block abstract}
This is the abstract
\BLOCK{endblock}

\BLOCK{block body}
\section{Introduction}

Traditionally, phylogenetic analyses have been performed by specialist software. A number of software packages exist that implement a broad but predefined collection of models and associated specialized inference methods. Typically, inference is handled by carefully crafted but computationally costly stochastic optimisation or Markov Chain Monte Carlo (MCMC) methods. In contrast, in other realms of statistical analysis, \textit{probabilistic programming} software libraries have entered into widespread use. These allow the specification of almost any model as a probabilistic program, and inference is provided automatically with a generic inference method. Having the power of probabilistic programming in phylogenetic analyses could significantly accelerate research. % TODO: Citations for software libraries

Probabilistic programming tools, such as BUGS \cite{lunn2000winbugs}, Stan \cite{carpenter2017stan}, PyMC3 \cite{salvatier2016probabilistic}, Pyro \cite{bingham2019pyro}, and TensorFlow Probability \cite{dillon2017tensorflow} allow users to specify probabilistic models by describing the generative process with code as a \textit{probabilistic program}. Advancements in automatic inference methods have allow these tools to perform efficient inference on almost any model. Some of notable examples, including Automatic Differentiation Variational Inference \cite{kucukelbir2017automatic} (ADVI) and Hamiltonian Monte Carlo \cite{duane1987hybrid}, use local information gradient information from the model's probability density function to efficiently navigate the parameter space.

One key technology that enables these gradient-based automatic inference algorithms is \textit{automatic differentiation}. Automatic differentiation refers to methods which efficiently calculate machine-precision gradients of functions, specified by computer programs, without extra analytical derivation or excessive computational overhead compared to the function evaluation. Automatic differentiation frameworks that have statistical inference packages built on top of them include Theano \cite{bergstra2010theano}, Pytorch \cite{paszke2019pytorch}, JAX \cite{jax2018github} and TensorFlow \cite{abadi2016tensorflow}. Some of these extend to non-trivial computational constructs such as control flow and recursion \cite{yu2018dynamic}.

The structure of the phylogenetic tree object is a major barrier to implementing probabilistic programming for phylogenetics. It is not clear how the association between its discrete and continuous quantities (the topology and branch lengths respectively) should be represented and handled in inference. Also, the combinatorial explosion of the size of the phylogenetic state space presents a major challenge to any inference algorithm. Generic random search methods for discrete variables, as in the naive implementation of MCMC sampling, do not scale appropriately to allow inference on modern datasets with thousands of sequences.

One core computation that presents a challenge to automatic differentiation is the \textit{phylogenetic likelihood}. This computes the probability function of a collection of sequences given a phylogenetic tree, integrating out the character states at ancestral nodes \cite{felsenstein1981evolutionary}. This is most efficiently performed through dynamic programming, which requires sequential control flow or recursion, which are not always trivial to implement in a functional automatic differentiation framework such as TensorFlow. Additionally, since the computational cost scales linearly with the number of sequences, naively computing the likelihood's gradient with respect to each branch of the tree yields a quadratic computational cost \cite{ji2020gradients}.


%% Existing work - VBPI, PhyloStan, PPHMC, zhang2020improved (normalising flows)

\section{Description}

TreeFlow is a library for probabilistic programming in Python. It is built on TensorFlow, a computational framework for machine learning. TreeFlow leverages Tensorflow's capabilities for accelerated numerical computation and automatic differentiation. It leverages the existing probabilistic modelling infrastructure provided by TensorFlow Probability, which implements standard statistical distributions and inferential machinery \cite{dillon2017tensorflow}. TreeFlow provides a phylogenetic tree representation for TensorFlow and associated input and output methods, a range of widely used phylogenetic distributions and functions, and tools for applying modern statistical inference methods to phylogenetic models.
% TODO: Flesh out this first paragraph

TensorFlow's core computational object is the Tensor, a multi-dimensional array with a uniform data type. Phylogenetic trees are not immediately at home in a Tensor-centric universe, as they are a complex data structure, often defined recursively, with both continuous and discrete components. TreeFlow represents trees as a structure of Tensors; floating point Tensors representing the times and branch lengths, and integer Tensors representing the topology. Common tree operations such as traversals and navigating to ancestor nodes can be performed through these integer Tensors. TensorFlow has extensive support for "nested" structures of Tensors, including using them as arguments to compiled functions and defining distributions over complex objects. This means computations and models involving phylogenetic trees can be expressed naturally.

A range of phylogenetic distributions and models are implemented in TreeFlow. These are primarily generative models for phylogenetic trees and models of sequence evolution. Generative models of phylogenetic trees, such as Kingman's coalescent \cite{kuhner1995estimating} and Birth-Death-Sampling speciation processes \cite{stadler2009incomplete}, can be used to infer parameters related to population dynamics from genetic sequence data. TreeFlow implements models of nucleotide sequence evolution such as the HKY85 \cite{hasegawa1985dating} and General Time Reversible (GTR) \cite{tavare1986some}. It includes a standard approach for dealing with heterogeneity in mutation rate across sites based on a marginalizing over a discretized site rate distribution \cite{yang1994maximum}. The probabilistic programming framework, however, allows for the use of any appropriate distribution as the base site rate distribution rather than just the standard single-parameter Gamma distribution. Thanks to TensorFlow's vectorized arithmetic operations, it is also natural to model variations in mutation rate over lineages by specifying parameters for multiple rates (possibly with a hierarchical prior) and multiplying by the branch lengths of the phylogenetic (time) tree. Models which can be naturally expressed this way include the log-Normal random local clock \cite{drummond2006relaxed} and auto-correlated relaxed clock models \cite{thorne1998estimating}.

These distributions can be composed into a *probabilistic graphical model* using TensorFlow Probability's joint distribution functionality \cite{piponi2020joint}.
% More on probabilistic programming


% Phylogenetic likelihood 
A computation that requires special treatment is the *phylogenetic likelihood*, the likelihood of a sequence alignment given a phylogeny and model of sequence evolution. This is typically involves integrating out character states at unsampled ancestral nodes using a *dynamic programming* computation known as *Felsenstein's pruning algorithm* \cite{felsenstein1981evolutionary}. The postorder tree traversal and dynamic data structure are not obviously compatible with Tensorflow's immutable data structures and focus on vectorized computations. Additionally, naive implementations result in gradient computations with problematic scaling. The computational cost of computing the derivatives of this likelihood with respect to all the branches of the phylogenetic tree could grow quadratically with respect to the number of taxa, and would prohibit gradient-based inference on large datasets \cite{ji2020gradients}. These issues are overcome in TreeFlow by implementing the dynamic programming data structure with TensorFlow's TensorArray construct \cite{yu2018dynamic}. The TensorArray is a data structure representing a collection of tensors which allows efficient implementation of the sequential computation. The *write-once* property enforced on its constituent tensors ensures that gradient computations have appropriate scaling, as evidenced by the benchmarks below.

% Inference
One form of statistical inference for which the gradient computation above is essential is variational Bayesian inference \cite{jordan1999introduction}. The goal of Bayesian inference is to characterise a *posterior distribution* which represents uncertainty over model parameters. Variational Bayesian inference achieves this by optimizing an approximation to the posterior distribution which has more convenient analytical properties. One concrete implementation of variational inference is *automatic differentiation variational inference* (ADVI) \cite{kucukelbir2017automatic}. ADVI can perform inference on a wide range of probabilistic graphical models composed of continuous variables. It automatically constructs an approximation to the posterior by transforming a convenient base distribution to respect the domains of the model's component distributions. It then optimizes this approximation using stochastic gradient methods \cite{robbins1951stochastic, bottou2010large}. TreeFlow implements ADVI using TensorFlow Probability's bijector framework to transform a base distribution and leverages the stochastic gradient optimizers already implemented in TensorFlow. It also opens the door to using TensorFlow's neural network framework to implement deep-learning-based variants of ADVI such as variational inference with *normalizing flows* \cite{rezende2015variational}, which transform the base distribution through invertible trainable neural network layers to better approximate complex posterior distributions.

% Ratio transform
Another useful tool for phylogenetic inference implemented in TreeFlow is the \textit{node height ratio transform} \cite{kishino2001performance}. The ratio transform parametrizes the internal node heights of a tree as the ratio between a node's height and its parent's height. The heights can be computed from the ratios in a pre-order tree traversal. This transformation has a triangular Jacobian matrix, which means computing the determinant required for change of variable of a probability density can be computed in linear time with respect to the number of internal node heights \cite{fourment2019evaluating}. In combination with a log transformation of the root height and a logit transformation of the ratios, a multivariate distribution that takes real values can be transformed into a distribution on internal node heights of rooted phylogenies. This has been applied to phylogenetic inference in the context of automatic differentiation variational inference \cite{fourment2019evaluating} and Hamiltonian Monte Carlo \cite{ji2021scalable}. The ratio transform is implemented using a TensorArray-based computation as a TensorFlow Probability Bijector which provides a convenient interface for transforming real-valued distributions into phylogenetic tree distributions. 

\section{Benchmarks}

\section{Results}


\BLOCK{endblock}